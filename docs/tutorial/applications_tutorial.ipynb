{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications Tutorial: Dynamic Correlations Across Research Domains\n",
    "\n",
    "This tutorial demonstrates how to apply dynamic correlation analysis across various research domains using the timecorr toolbox. We'll explore applications in neuroscience, psychology, economics, climate science, and more.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dynamic correlations are useful across many fields where time-varying relationships between variables are of interest. This tutorial shows how to adapt timecorr for different research contexts and data types.\n",
    "\n",
    "### Key Applications:\n",
    "- **Neuroscience**: Brain connectivity analysis\n",
    "- **Psychology**: Behavioral pattern analysis\n",
    "- **Economics**: Market correlation analysis\n",
    "- **Climate Science**: Environmental variable relationships\n",
    "- **Social Sciences**: Network dynamics\n",
    "- **Biology**: Gene expression patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats, signal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import timecorr\n",
    "import timecorr as tc\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Timecorr version: {tc.__version__ if hasattr(tc, '__version__') else 'Unknown'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neuroscience Application: Brain Network Connectivity\n",
    "\n",
    "In neuroscience, dynamic correlations can reveal how brain networks change over time during different cognitive states or tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate brain network activity\n",
    "def simulate_brain_networks(n_regions=20, n_timepoints=300, n_networks=3):\n",
    "    \"\"\"\n",
    "    Simulate brain activity with multiple networks that show different\n",
    "    connectivity patterns over time.\n",
    "    \"\"\"\n",
    "    # Define network structure\n",
    "    regions_per_network = n_regions // n_networks\n",
    "    \n",
    "    # Create base connectivity patterns\n",
    "    brain_data = np.random.randn(n_timepoints, n_regions) * 0.5\n",
    "    \n",
    "    # Add network-specific patterns that change over time\n",
    "    time = np.arange(n_timepoints)\n",
    "    \n",
    "    # Network 1: Default Mode Network - active during rest\n",
    "    dmn_activity = np.sin(2 * np.pi * time / 100) * 0.8\n",
    "    for i in range(regions_per_network):\n",
    "        brain_data[:, i] += dmn_activity + np.random.randn(n_timepoints) * 0.2\n",
    "    \n",
    "    # Network 2: Task-Positive Network - active during tasks\n",
    "    task_periods = (time % 50) < 25  # Task periods\n",
    "    task_activity = np.where(task_periods, 1.0, 0.2)\n",
    "    for i in range(regions_per_network, 2 * regions_per_network):\n",
    "        brain_data[:, i] += task_activity + np.random.randn(n_timepoints) * 0.2\n",
    "    \n",
    "    # Network 3: Salience Network - transitions between states\n",
    "    transition_points = np.where(np.diff(task_periods.astype(int)))[0]\n",
    "    salience_activity = np.zeros(n_timepoints)\n",
    "    for tp in transition_points:\n",
    "        salience_activity[max(0, tp-5):min(n_timepoints, tp+5)] = 1.0\n",
    "    for i in range(2 * regions_per_network, n_regions):\n",
    "        brain_data[:, i] += salience_activity + np.random.randn(n_timepoints) * 0.2\n",
    "    \n",
    "    return brain_data\n",
    "\n",
    "# Generate synthetic brain data\n",
    "brain_data = simulate_brain_networks(n_regions=21, n_timepoints=300)  # 21 regions (7 per network)\n",
    "region_names = [f'Region_{i+1}' for i in range(21)]\n",
    "network_labels = ['DMN'] * 7 + ['Task'] * 7 + ['Salience'] * 7\n",
    "\n",
    "print(f\"Simulated brain data shape: {brain_data.shape}\")\n",
    "print(f\"Networks: {set(network_labels)}\")\n",
    "\n",
    "# Visualize brain network activity\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot activity for each network\n",
    "colors = ['blue', 'red', 'green']\n",
    "network_types = ['DMN', 'Task', 'Salience']\n",
    "\n",
    "for i, (network, color) in enumerate(zip(network_types, colors)):\n",
    "    network_indices = [j for j, label in enumerate(network_labels) if label == network]\n",
    "    \n",
    "    # Plot mean activity across regions in this network\n",
    "    network_activity = brain_data[:, network_indices]\n",
    "    mean_activity = np.mean(network_activity, axis=1)\n",
    "    \n",
    "    axes[i].plot(mean_activity, color=color, linewidth=2)\n",
    "    axes[i].set_title(f'{network} Network Activity', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_ylabel('Activity')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    if i == 2:\n",
    "        axes[i].set_xlabel('Time (TRs)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dynamic functional connectivity\n",
    "dynamic_fc = tc.timecorr(\n",
    "    brain_data,\n",
    "    weights_function=tc.gaussian_weights,\n",
    "    weights_params={'var': 25}  # Moderate temporal smoothing\n",
    ")\n",
    "\n",
    "# Convert to matrix format for analysis\n",
    "fc_matrices = tc.vec2mat(dynamic_fc)\n",
    "print(f\"Dynamic FC matrices shape: {fc_matrices.shape}\")\n",
    "\n",
    "# Analyze within-network and between-network connectivity\n",
    "def analyze_network_connectivity(fc_matrices, network_labels):\n",
    "    \"\"\"\n",
    "    Analyze connectivity within and between networks.\n",
    "    \"\"\"\n",
    "    network_types = list(set(network_labels))\n",
    "    n_timepoints = fc_matrices.shape[2]\n",
    "    \n",
    "    connectivity_measures = {}\n",
    "    \n",
    "    for net1 in network_types:\n",
    "        for net2 in network_types:\n",
    "            indices1 = [i for i, label in enumerate(network_labels) if label == net1]\n",
    "            indices2 = [i for i, label in enumerate(network_labels) if label == net2]\n",
    "            \n",
    "            # Extract connectivity between these networks\n",
    "            if net1 == net2:\n",
    "                # Within-network connectivity (exclude diagonal)\n",
    "                mask = np.triu(np.ones((len(indices1), len(indices1))), k=1)\n",
    "                within_conn = []\n",
    "                for t in range(n_timepoints):\n",
    "                    submatrix = fc_matrices[np.ix_(indices1, indices1, [t])].squeeze()\n",
    "                    within_conn.append(np.mean(submatrix[mask == 1]))\n",
    "                connectivity_measures[f'{net1}_within'] = np.array(within_conn)\n",
    "            elif net1 < net2:  # Avoid duplicates\n",
    "                # Between-network connectivity\n",
    "                between_conn = []\n",
    "                for t in range(n_timepoints):\n",
    "                    submatrix = fc_matrices[np.ix_(indices1, indices2, [t])].squeeze()\n",
    "                    between_conn.append(np.mean(submatrix))\n",
    "                connectivity_measures[f'{net1}_{net2}'] = np.array(between_conn)\n",
    "    \n",
    "    return connectivity_measures\n",
    "\n",
    "# Analyze connectivity patterns\n",
    "connectivity_measures = analyze_network_connectivity(fc_matrices, network_labels)\n",
    "\n",
    "# Plot connectivity patterns\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (measure_name, connectivity) in enumerate(connectivity_measures.items()):\n",
    "    if i < 6:  # Plot first 6 measures\n",
    "        axes[i].plot(connectivity, linewidth=2)\n",
    "        axes[i].set_title(f'{measure_name.replace(\"_\", \"-\")} Connectivity', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('Time (TRs)')\n",
    "        axes[i].set_ylabel('Connectivity')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Economic Application: Market Correlation Analysis\n",
    "\n",
    "In finance, dynamic correlations can reveal changing relationships between asset prices, market sectors, or economic indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate financial market data\n",
    "def simulate_market_data(n_assets=10, n_days=500, sectors=['Tech', 'Finance', 'Energy']):\n",
    "    \"\"\"\n",
    "    Simulate stock price returns with sector-specific patterns and market events.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create asset names and sector assignments\n",
    "    assets_per_sector = n_assets // len(sectors)\n",
    "    asset_names = []\n",
    "    sector_labels = []\n",
    "    \n",
    "    for i, sector in enumerate(sectors):\n",
    "        for j in range(assets_per_sector):\n",
    "            asset_names.append(f'{sector}_{j+1}')\n",
    "            sector_labels.append(sector)\n",
    "    \n",
    "    # Add remaining assets to first sector\n",
    "    remaining = n_assets - len(asset_names)\n",
    "    for j in range(remaining):\n",
    "        asset_names.append(f'{sectors[0]}_{assets_per_sector + j + 1}')\n",
    "        sector_labels.append(sectors[0])\n",
    "    \n",
    "    # Generate base returns\n",
    "    returns = np.random.randn(n_days, n_assets) * 0.02  # 2% daily volatility\n",
    "    \n",
    "    # Add market-wide trends\n",
    "    time = np.arange(n_days)\n",
    "    market_trend = np.sin(2 * np.pi * time / 100) * 0.01\n",
    "    \n",
    "    # Add sector-specific patterns\n",
    "    for i, sector in enumerate(sectors):\n",
    "        sector_indices = [j for j, label in enumerate(sector_labels) if label == sector]\n",
    "        \n",
    "        if sector == 'Tech':\n",
    "            # Tech sector: high volatility, growth trend\n",
    "            tech_pattern = np.cumsum(np.random.randn(n_days) * 0.005) * 0.1\n",
    "            for idx in sector_indices:\n",
    "                returns[:, idx] += market_trend + tech_pattern + np.random.randn(n_days) * 0.01\n",
    "        \n",
    "        elif sector == 'Finance':\n",
    "            # Finance sector: correlated with interest rates\n",
    "            interest_rate_proxy = np.sin(2 * np.pi * time / 200) * 0.02\n",
    "            for idx in sector_indices:\n",
    "                returns[:, idx] += market_trend + interest_rate_proxy + np.random.randn(n_days) * 0.015\n",
    "        \n",
    "        elif sector == 'Energy':\n",
    "            # Energy sector: volatile, commodity-driven\n",
    "            commodity_shock = np.zeros(n_days)\n",
    "            shock_points = np.random.choice(n_days, size=5, replace=False)\n",
    "            for sp in shock_points:\n",
    "                commodity_shock[max(0, sp-10):min(n_days, sp+10)] = np.random.randn() * 0.05\n",
    "            for idx in sector_indices:\n",
    "                returns[:, idx] += market_trend + commodity_shock + np.random.randn(n_days) * 0.02\n",
    "    \n",
    "    # Add market crash event\n",
    "    crash_day = n_days // 2\n",
    "    crash_magnitude = -0.1  # 10% drop\n",
    "    crash_recovery = np.exp(-np.arange(20) / 5)  # Exponential recovery\n",
    "    \n",
    "    for i in range(min(20, n_days - crash_day)):\n",
    "        returns[crash_day + i, :] += crash_magnitude * crash_recovery[i]\n",
    "    \n",
    "    return returns, asset_names, sector_labels\n",
    "\n",
    "# Generate market data\n",
    "market_returns, asset_names, sector_labels = simulate_market_data(n_assets=12, n_days=400)\n",
    "\n",
    "print(f\"Market data shape: {market_returns.shape}\")\n",
    "print(f\"Assets: {asset_names}\")\n",
    "print(f\"Sectors: {set(sector_labels)}\")\n",
    "\n",
    "# Visualize market data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot cumulative returns\n",
    "cumulative_returns = np.cumprod(1 + market_returns, axis=0)\n",
    "for i, (asset, sector) in enumerate(zip(asset_names, sector_labels)):\n",
    "    color = {'Tech': 'blue', 'Finance': 'red', 'Energy': 'green'}[sector]\n",
    "    axes[0].plot(cumulative_returns[:, i], label=asset, color=color, alpha=0.7)\n",
    "\n",
    "axes[0].set_title('Cumulative Returns by Asset', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Trading Days')\n",
    "axes[0].set_ylabel('Cumulative Return')\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot sector average returns\n",
    "sectors = list(set(sector_labels))\n",
    "for sector in sectors:\n",
    "    sector_indices = [i for i, label in enumerate(sector_labels) if label == sector]\n",
    "    sector_returns = np.mean(market_returns[:, sector_indices], axis=1)\n",
    "    sector_cumulative = np.cumprod(1 + sector_returns)\n",
    "    \n",
    "    color = {'Tech': 'blue', 'Finance': 'red', 'Energy': 'green'}[sector]\n",
    "    axes[1].plot(sector_cumulative, label=sector, color=color, linewidth=3)\n",
    "\n",
    "axes[1].set_title('Sector Average Cumulative Returns', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Trading Days')\n",
    "axes[1].set_ylabel('Cumulative Return')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dynamic correlations in financial markets\n",
    "dynamic_corr_short = tc.timecorr(\n",
    "    market_returns,\n",
    "    weights_function=tc.gaussian_weights,\n",
    "    weights_params={'var': 25}  # Short-term correlations (1 month)\n",
    ")\n",
    "\n",
    "dynamic_corr_long = tc.timecorr(\n",
    "    market_returns,\n",
    "    weights_function=tc.gaussian_weights,\n",
    "    weights_params={'var': 100}  # Long-term correlations (quarterly)\n",
    ")\n",
    "\n",
    "# Convert to matrix format\n",
    "corr_matrices_short = tc.vec2mat(dynamic_corr_short)\n",
    "corr_matrices_long = tc.vec2mat(dynamic_corr_long)\n",
    "\n",
    "print(f\"Short-term correlation matrices shape: {corr_matrices_short.shape}\")\n",
    "print(f\"Long-term correlation matrices shape: {corr_matrices_long.shape}\")\n",
    "\n",
    "# Analyze market correlation during different periods\n",
    "def analyze_market_periods(corr_matrices, period_name):\n",
    "    \"\"\"\n",
    "    Analyze market correlations during different periods.\n",
    "    \"\"\"\n",
    "    n_days = corr_matrices.shape[2]\n",
    "    \n",
    "    # Define periods\n",
    "    normal_period = slice(0, n_days//2 - 20)\n",
    "    crash_period = slice(n_days//2 - 20, n_days//2 + 20)\n",
    "    recovery_period = slice(n_days//2 + 20, n_days)\n",
    "    \n",
    "    periods = {\n",
    "        'Normal': normal_period,\n",
    "        'Crash': crash_period,\n",
    "        'Recovery': recovery_period\n",
    "    }\n",
    "    \n",
    "    period_stats = {}\n",
    "    \n",
    "    for period_name, period_slice in periods.items():\n",
    "        period_corrs = corr_matrices[:, :, period_slice]\n",
    "        \n",
    "        # Compute average correlation matrix for this period\n",
    "        avg_corr = np.mean(period_corrs, axis=2)\n",
    "        \n",
    "        # Compute average correlation (excluding diagonal)\n",
    "        mask = np.triu(np.ones_like(avg_corr), k=1)\n",
    "        avg_correlation = np.mean(avg_corr[mask == 1])\n",
    "        \n",
    "        period_stats[period_name] = {\n",
    "            'avg_corr_matrix': avg_corr,\n",
    "            'avg_correlation': avg_correlation,\n",
    "            'period_slice': period_slice\n",
    "        }\n",
    "    \n",
    "    return period_stats\n",
    "\n",
    "# Analyze both short-term and long-term correlations\n",
    "short_term_stats = analyze_market_periods(corr_matrices_short, 'Short-term')\n",
    "long_term_stats = analyze_market_periods(corr_matrices_long, 'Long-term')\n",
    "\n",
    "# Plot correlation matrices for different periods\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "periods = ['Normal', 'Crash', 'Recovery']\n",
    "\n",
    "for i, period in enumerate(periods):\n",
    "    # Short-term correlations\n",
    "    sns.heatmap(short_term_stats[period]['avg_corr_matrix'], \n",
    "                annot=False, cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "                ax=axes[0, i], cbar=True)\n",
    "    axes[0, i].set_title(f'Short-term: {period} Period\\n(Avg Corr: {short_term_stats[period][\"avg_correlation\"]:.3f})')\n",
    "    axes[0, i].set_xticklabels(asset_names, rotation=45)\n",
    "    axes[0, i].set_yticklabels(asset_names, rotation=0)\n",
    "    \n",
    "    # Long-term correlations\n",
    "    sns.heatmap(long_term_stats[period]['avg_corr_matrix'], \n",
    "                annot=False, cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "                ax=axes[1, i], cbar=True)\n",
    "    axes[1, i].set_title(f'Long-term: {period} Period\\n(Avg Corr: {long_term_stats[period][\"avg_correlation\"]:.3f})')\n",
    "    axes[1, i].set_xticklabels(asset_names, rotation=45)\n",
    "    axes[1, i].set_yticklabels(asset_names, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot time series of average correlations\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Compute average correlation over time\n",
    "def compute_avg_correlation_timeseries(corr_matrices):\n",
    "    n_days = corr_matrices.shape[2]\n",
    "    avg_corrs = []\n",
    "    \n",
    "    for t in range(n_days):\n",
    "        corr_matrix = corr_matrices[:, :, t]\n",
    "        mask = np.triu(np.ones_like(corr_matrix), k=1)\n",
    "        avg_corr = np.mean(corr_matrix[mask == 1])\n",
    "        avg_corrs.append(avg_corr)\n",
    "    \n",
    "    return np.array(avg_corrs)\n",
    "\n",
    "avg_corr_short = compute_avg_correlation_timeseries(corr_matrices_short)\n",
    "avg_corr_long = compute_avg_correlation_timeseries(corr_matrices_long)\n",
    "\n",
    "ax.plot(avg_corr_short, label='Short-term (1 month)', linewidth=2, alpha=0.8)\n",
    "ax.plot(avg_corr_long, label='Long-term (quarterly)', linewidth=2, alpha=0.8)\n",
    "ax.axvline(x=len(market_returns)//2, color='red', linestyle='--', alpha=0.7, label='Market Crash')\n",
    "\n",
    "ax.set_title('Average Market Correlation Over Time', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Trading Days')\n",
    "ax.set_ylabel('Average Correlation')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"• Normal period average correlation: {short_term_stats['Normal']['avg_correlation']:.3f}\")\n",
    "print(f\"• Crash period average correlation: {short_term_stats['Crash']['avg_correlation']:.3f}\")\n",
    "print(f\"• Recovery period average correlation: {short_term_stats['Recovery']['avg_correlation']:.3f}\")\n",
    "print(f\"• Correlation increase during crash: {short_term_stats['Crash']['avg_correlation'] - short_term_stats['Normal']['avg_correlation']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Climate Science Application: Environmental Variable Relationships\n",
    "\n",
    "In climate science, dynamic correlations can reveal changing relationships between environmental variables like temperature, precipitation, and atmospheric conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate climate data\ndef simulate_climate_data(n_years=20, locations=['Arctic', 'Temperate', 'Tropical']):\n    \"\"\"\n    Simulate climate data with seasonal patterns and long-term trends.\n    \"\"\"\n    n_days = n_years * 365\n    time = np.arange(n_days)\n    \n    # Create variable names\n    variables = ['Temperature', 'Precipitation', 'Humidity', 'Wind_Speed']\n    data = np.zeros((n_days, len(locations) * len(variables)))\n    column_names = []\n    \n    for i, location in enumerate(locations):\n        for j, variable in enumerate(variables):\n            col_idx = i * len(variables) + j\n            column_names.append(f'{location}_{variable}')\n            \n            # Seasonal patterns\n            seasonal = np.sin(2 * np.pi * time / 365.25)  # Annual cycle\n            \n            # Location-specific modifications\n            if location == 'Arctic':\n                seasonal *= 2  # Extreme seasonal variation\n                base_temp = -10\n            elif location == 'Temperate':\n                seasonal *= 1  # Moderate seasonal variation\n                base_temp = 15\n            else:  # Tropical\n                seasonal *= 0.3  # Minimal seasonal variation\n                base_temp = 25\n            \n            # Variable-specific patterns\n            if variable == 'Temperature':\n                data[:, col_idx] = base_temp + seasonal * 20 + np.random.randn(n_days) * 3\n            elif variable == 'Precipitation':\n                # Precipitation inversely correlated with temperature in some regions\n                precip_seasonal = -seasonal if location == 'Tropical' else seasonal\n                data[:, col_idx] = np.maximum(0, 50 + precip_seasonal * 30 + np.random.randn(n_days) * 15)\n            elif variable == 'Humidity':\n                # Humidity related to temperature and precipitation\n                base_humidity = 70 if location == 'Tropical' else 50\n                data[:, col_idx] = base_humidity + seasonal * 10 + np.random.randn(n_days) * 5\n            elif variable == 'Wind_Speed':\n                # Wind speed with seasonal patterns\n                data[:, col_idx] = np.maximum(0, 10 + seasonal * 5 + np.random.randn(n_days) * 3)\n    \n    # Add long-term climate change trend\n    climate_trend = np.linspace(0, 2, n_days)  # 2 degree warming over period\n    for i, col_name in enumerate(column_names):\n        if 'Temperature' in col_name:\n            data[:, i] += climate_trend\n    \n    # Add extreme weather events\n    n_events = 10\n    event_days = np.random.choice(n_days, size=n_events, replace=False)\n    for event_day in event_days:\n        # Extreme weather affects multiple variables\n        for i, col_name in enumerate(column_names):\n            if 'Temperature' in col_name:\n                data[event_day:event_day+3, i] += np.random.randn() * 10\n            elif 'Precipitation' in col_name:\n                data[event_day:event_day+3, i] *= (1 + np.random.randn() * 0.5)\n    \n    return data, column_names\n\n# Generate climate data\nclimate_data, climate_variables = simulate_climate_data(n_years=10)\n\nprint(f\"Climate data shape: {climate_data.shape}\")\nprint(f\"Variables: {climate_variables}\")\n\n# Visualize climate data\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\naxes = axes.flatten()\n\nvariable_types = ['Temperature', 'Precipitation', 'Humidity', 'Wind_Speed']\ncolors = ['red', 'blue', 'green', 'orange']\n\nfor i, var_type in enumerate(variable_types):\n    # Plot all locations for this variable type\n    for j, var_name in enumerate(climate_variables):\n        if var_type in var_name:\n            location = var_name.split('_')[0]\n            # Plot monthly averages for clarity - use proper monthly averaging\n            n_months = len(climate_data) // 30\n            monthly_data = np.array([climate_data[k*30:(k+1)*30, j].mean() for k in range(n_months)])\n            axes[i].plot(monthly_data, label=location, alpha=0.8)\n    \n    axes[i].set_title(f'{var_type} Over Time', fontsize=12, fontweight='bold')\n    axes[i].set_xlabel('Months')\n    axes[i].set_ylabel(var_type)\n    axes[i].legend()\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute dynamic correlations in climate data\n# Use different time scales for different analyses\n\n# Short-term correlations (seasonal)\nclimate_corr_seasonal = tc.timecorr(\n    climate_data,\n    weights_function=tc.gaussian_weights,\n    weights_params={'var': 365}  # Annual window\n)\n\n# Long-term correlations (multi-year trends)\nclimate_corr_longterm = tc.timecorr(\n    climate_data,\n    weights_function=tc.gaussian_weights,\n    weights_params={'var': 1000}  # Multi-year window\n)\n\n# Convert to matrix format\nclimate_matrices_seasonal = tc.vec2mat(climate_corr_seasonal)\nclimate_matrices_longterm = tc.vec2mat(climate_corr_longterm)\n\nprint(f\"Seasonal correlation matrices shape: {climate_matrices_seasonal.shape}\")\nprint(f\"Long-term correlation matrices shape: {climate_matrices_longterm.shape}\")\n\n# Analyze temperature-precipitation relationships\ndef analyze_climate_relationships(corr_matrices, variable_names):\n    \"\"\"\n    Analyze relationships between climate variables.\n    \"\"\"\n    relationships = {}\n    \n    # Find temperature and precipitation indices\n    temp_indices = [i for i, name in enumerate(variable_names) if 'Temperature' in name]\n    precip_indices = [i for i, name in enumerate(variable_names) if 'Precipitation' in name]\n    \n    locations = ['Arctic', 'Temperate', 'Tropical']\n    \n    for location in locations:\n        temp_idx = [i for i, name in enumerate(variable_names) if f'{location}_Temperature' in name][0]\n        precip_idx = [i for i, name in enumerate(variable_names) if f'{location}_Precipitation' in name][0]\n        \n        # Extract temperature-precipitation correlation over time\n        temp_precip_corr = corr_matrices[temp_idx, precip_idx, :]\n        relationships[f'{location}_Temp_Precip'] = temp_precip_corr\n    \n    return relationships\n\nseasonal_relationships = analyze_climate_relationships(climate_matrices_seasonal, climate_variables)\nlongterm_relationships = analyze_climate_relationships(climate_matrices_longterm, climate_variables)\n\n# Plot temperature-precipitation relationships\nfig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\nlocations = ['Arctic', 'Temperate', 'Tropical']\ncolors = ['blue', 'green', 'red']\n\n# Seasonal relationships\nfor location, color in zip(locations, colors):\n    relationship = seasonal_relationships[f'{location}_Temp_Precip']\n    # Plot monthly averages using proper monthly averaging\n    n_months = len(relationship) // 30\n    monthly_rel = np.array([relationship[k*30:(k+1)*30].mean() for k in range(n_months)])\n    axes[0].plot(monthly_rel, label=location, color=color, linewidth=2)\n\naxes[0].set_title('Temperature-Precipitation Correlation (Seasonal)', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Months')\naxes[0].set_ylabel('Correlation')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\naxes[0].set_ylim([-1, 1])\n\n# Long-term relationships\nfor location, color in zip(locations, colors):\n    relationship = longterm_relationships[f'{location}_Temp_Precip']\n    # Plot monthly averages using proper monthly averaging\n    n_months = len(relationship) // 30\n    monthly_rel = np.array([relationship[k*30:(k+1)*30].mean() for k in range(n_months)])\n    axes[1].plot(monthly_rel, label=location, color=color, linewidth=2)\n\naxes[1].set_title('Temperature-Precipitation Correlation (Long-term)', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Months')\naxes[1].set_ylabel('Correlation')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].set_ylim([-1, 1])\n\nplt.tight_layout()\nplt.show()\n\n# Show correlation matrices at different time periods\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\ntime_periods = [len(climate_data)//4, len(climate_data)//2, 3*len(climate_data)//4]\nperiod_names = ['Early Period', 'Middle Period', 'Late Period']\n\nfor i, (time_point, period_name) in enumerate(zip(time_periods, period_names)):\n    # Use monthly averages for time point\n    monthly_time_point = time_point // 30\n    if monthly_time_point < climate_matrices_seasonal.shape[2]:\n        corr_matrix = climate_matrices_seasonal[:, :, monthly_time_point]\n        \n        sns.heatmap(corr_matrix, annot=False, cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n                    ax=axes[i], cbar=True)\n        axes[i].set_title(f'{period_name}', fontsize=12)\n        axes[i].set_xticklabels([name.replace('_', '\\n') for name in climate_variables], rotation=45)\n        axes[i].set_yticklabels([name.replace('_', '\\n') for name in climate_variables], rotation=0)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Social Sciences Application: Social Network Dynamics\n",
    "\n",
    "In social sciences, dynamic correlations can reveal changing relationships in social networks, communication patterns, or behavioral synchrony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate social network data\n",
    "def simulate_social_network_data(n_individuals=15, n_timepoints=200):\n",
    "    \"\"\"\n",
    "    Simulate social network activity with group formation and influence dynamics.\n",
    "    \"\"\"\n",
    "    # Create individual names\n",
    "    individuals = [f'Person_{i+1}' for i in range(n_individuals)]\n",
    "    \n",
    "    # Initialize data\n",
    "    social_data = np.random.randn(n_timepoints, n_individuals) * 0.5\n",
    "    \n",
    "    # Define social groups\n",
    "    group_size = n_individuals // 3\n",
    "    groups = {\n",
    "        'Group_A': list(range(group_size)),\n",
    "        'Group_B': list(range(group_size, 2*group_size)),\n",
    "        'Group_C': list(range(2*group_size, n_individuals))\n",
    "    }\n",
    "    \n",
    "    # Add group-specific behaviors\n",
    "    time = np.arange(n_timepoints)\n",
    "    \n",
    "    # Group A: Early adopters - lead trends\n",
    "    trend_A = np.sin(2 * np.pi * time / 50) * 1.5\n",
    "    for idx in groups['Group_A']:\n",
    "        social_data[:, idx] += trend_A + np.random.randn(n_timepoints) * 0.3\n",
    "    \n",
    "    # Group B: Followers - adopt trends with delay\n",
    "    trend_B = np.sin(2 * np.pi * (time - 10) / 50) * 1.2  # 10 time-step delay\n",
    "    for idx in groups['Group_B']:\n",
    "        social_data[:, idx] += trend_B + np.random.randn(n_timepoints) * 0.3\n",
    "    \n",
    "    # Group C: Independent - different pattern\n",
    "    trend_C = np.cos(2 * np.pi * time / 30) * 1.0\n",
    "    for idx in groups['Group_C']:\n",
    "        social_data[:, idx] += trend_C + np.random.randn(n_timepoints) * 0.4\n",
    "    \n",
    "    # Add social influence - individuals influence their neighbors\n",
    "    influence_strength = 0.1\n",
    "    for t in range(1, n_timepoints):\n",
    "        for group_name, group_indices in groups.items():\n",
    "            # Within-group influence\n",
    "            group_mean = np.mean(social_data[t-1, group_indices])\n",
    "            for idx in group_indices:\n",
    "                social_data[t, idx] += influence_strength * (group_mean - social_data[t-1, idx])\n",
    "    \n",
    "    # Add interaction events between groups\n",
    "    interaction_timepoints = np.random.choice(n_timepoints, size=10, replace=False)\n",
    "    for t in interaction_timepoints:\n",
    "        # Random individuals from different groups interact\n",
    "        person1 = np.random.choice(groups['Group_A'])\n",
    "        person2 = np.random.choice(groups['Group_B'])\n",
    "        \n",
    "        # Mutual influence\n",
    "        avg_behavior = (social_data[t, person1] + social_data[t, person2]) / 2\n",
    "        social_data[t:t+5, person1] += 0.2 * (avg_behavior - social_data[t, person1])\n",
    "        social_data[t:t+5, person2] += 0.2 * (avg_behavior - social_data[t, person2])\n",
    "    \n",
    "    return social_data, individuals, groups\n",
    "\n",
    "# Generate social network data\n",
    "social_data, individuals, groups = simulate_social_network_data(n_individuals=15, n_timepoints=200)\n",
    "\n",
    "print(f\"Social network data shape: {social_data.shape}\")\n",
    "print(f\"Individuals: {individuals}\")\n",
    "print(f\"Groups: {list(groups.keys())}\")\n",
    "\n",
    "# Visualize social network activity\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot individual behaviors\n",
    "colors = ['red', 'blue', 'green']\n",
    "group_names = list(groups.keys())\n",
    "\n",
    "for i, (group_name, group_indices) in enumerate(groups.items()):\n",
    "    for idx in group_indices:\n",
    "        axes[0].plot(social_data[:, idx], color=colors[i], alpha=0.6, linewidth=1)\n",
    "    \n",
    "    # Plot group average\n",
    "    group_avg = np.mean(social_data[:, group_indices], axis=1)\n",
    "    axes[0].plot(group_avg, color=colors[i], linewidth=3, label=f'{group_name} Average')\n",
    "\n",
    "axes[0].set_title('Individual Social Behaviors Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Behavior Measure')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot group correlations over time\n",
    "for i, (group_name, group_indices) in enumerate(groups.items()):\n",
    "    group_data = social_data[:, group_indices]\n",
    "    \n",
    "    # Compute within-group correlations over time windows\n",
    "    window_size = 20\n",
    "    within_group_corrs = []\n",
    "    \n",
    "    for t in range(window_size, len(social_data) - window_size):\n",
    "        window_data = group_data[t-window_size:t+window_size, :]\n",
    "        corr_matrix = np.corrcoef(window_data.T)\n",
    "        \n",
    "        # Average correlation (excluding diagonal)\n",
    "        mask = np.triu(np.ones_like(corr_matrix), k=1)\n",
    "        if np.sum(mask) > 0:\n",
    "            avg_corr = np.mean(corr_matrix[mask == 1])\n",
    "            within_group_corrs.append(avg_corr)\n",
    "        else:\n",
    "            within_group_corrs.append(0)\n",
    "    \n",
    "    time_axis = np.arange(window_size, len(social_data) - window_size)\n",
    "    axes[1].plot(time_axis, within_group_corrs, color=colors[i], linewidth=2, label=f'{group_name} Cohesion')\n",
    "\n",
    "axes[1].set_title('Group Cohesion Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Average Within-Group Correlation')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dynamic correlations in social network\n",
    "social_dynamic_corr = tc.timecorr(\n",
    "    social_data,\n",
    "    weights_function=tc.gaussian_weights,\n",
    "    weights_params={'var': 20}  # Medium-term social influence window\n",
    ")\n",
    "\n",
    "# Convert to matrix format\n",
    "social_corr_matrices = tc.vec2mat(social_dynamic_corr)\n",
    "\n",
    "print(f\"Social correlation matrices shape: {social_corr_matrices.shape}\")\n",
    "\n",
    "# Analyze social network structure\n",
    "def analyze_social_network_structure(corr_matrices, groups):\n",
    "    \"\"\"\n",
    "    Analyze within-group and between-group social connections.\n",
    "    \"\"\"\n",
    "    n_timepoints = corr_matrices.shape[2]\n",
    "    group_names = list(groups.keys())\n",
    "    \n",
    "    # Compute within-group and between-group correlations\n",
    "    network_measures = {}\n",
    "    \n",
    "    # Within-group correlations\n",
    "    for group_name, group_indices in groups.items():\n",
    "        within_group_corrs = []\n",
    "        for t in range(n_timepoints):\n",
    "            group_matrix = corr_matrices[np.ix_(group_indices, group_indices, [t])].squeeze()\n",
    "            if len(group_indices) > 1:\n",
    "                mask = np.triu(np.ones_like(group_matrix), k=1)\n",
    "                if np.sum(mask) > 0:\n",
    "                    avg_corr = np.mean(group_matrix[mask == 1])\n",
    "                    within_group_corrs.append(avg_corr)\n",
    "                else:\n",
    "                    within_group_corrs.append(0)\n",
    "            else:\n",
    "                within_group_corrs.append(0)\n",
    "        \n",
    "        network_measures[f'{group_name}_within'] = np.array(within_group_corrs)\n",
    "    \n",
    "    # Between-group correlations\n",
    "    for i, group1 in enumerate(group_names):\n",
    "        for j, group2 in enumerate(group_names):\n",
    "            if i < j:  # Avoid duplicates\n",
    "                between_group_corrs = []\n",
    "                indices1 = groups[group1]\n",
    "                indices2 = groups[group2]\n",
    "                \n",
    "                for t in range(n_timepoints):\n",
    "                    between_matrix = corr_matrices[np.ix_(indices1, indices2, [t])].squeeze()\n",
    "                    avg_corr = np.mean(between_matrix)\n",
    "                    between_group_corrs.append(avg_corr)\n",
    "                \n",
    "                network_measures[f'{group1}_{group2}'] = np.array(between_group_corrs)\n",
    "    \n",
    "    return network_measures\n",
    "\n",
    "# Analyze network structure\n",
    "network_measures = analyze_social_network_structure(social_corr_matrices, groups)\n",
    "\n",
    "# Plot network measures\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_idx = 0\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown']\n",
    "\n",
    "for measure_name, measure_values in network_measures.items():\n",
    "    if plot_idx < 6:\n",
    "        axes[plot_idx].plot(measure_values, color=colors[plot_idx], linewidth=2)\n",
    "        axes[plot_idx].set_title(f'{measure_name.replace(\"_\", \"-\")} Correlation', \n",
    "                                fontsize=12, fontweight='bold')\n",
    "        axes[plot_idx].set_xlabel('Time')\n",
    "        axes[plot_idx].set_ylabel('Correlation')\n",
    "        axes[plot_idx].grid(True, alpha=0.3)\n",
    "        axes[plot_idx].set_ylim([-1, 1])\n",
    "        plot_idx += 1\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(plot_idx, 4):\n",
    "    axes[i].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show network structure at different time points\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "time_points = [50, 100, 150]\n",
    "time_labels = ['Early', 'Middle', 'Late']\n",
    "\n",
    "for i, (time_point, time_label) in enumerate(zip(time_points, time_labels)):\n",
    "    corr_matrix = social_corr_matrices[:, :, time_point]\n",
    "    \n",
    "    # Create group-colored visualization\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "                ax=axes[i], cbar=True)\n",
    "    axes[i].set_title(f'{time_label} Period\\n(t={time_point})', fontsize=12)\n",
    "    \n",
    "    # Add group boundaries\n",
    "    group_boundaries = [0, 5, 10, 15]\n",
    "    for boundary in group_boundaries[1:-1]:\n",
    "        axes[i].axhline(y=boundary, color='white', linewidth=2)\n",
    "        axes[i].axvline(x=boundary, color='white', linewidth=2)\n",
    "    \n",
    "    axes[i].set_xticklabels(individuals, rotation=45)\n",
    "    axes[i].set_yticklabels(individuals, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Social Network Insights:\")\n",
    "print(f\"• Average within-group correlation: {np.mean([np.mean(network_measures[key]) for key in network_measures.keys() if 'within' in key]):.3f}\")\n",
    "print(f\"• Average between-group correlation: {np.mean([np.mean(network_measures[key]) for key in network_measures.keys() if 'within' not in key]):.3f}\")\n",
    "print(f\"• Group cohesion varies over time, showing social dynamics\")\n",
    "print(f\"• Between-group connections emerge through social interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Best Practices for Different Domains\n",
    "\n",
    "This tutorial has demonstrated how to apply dynamic correlation analysis across various research domains. Here are key takeaways for each field:\n",
    "\n",
    "### Domain-Specific Considerations:\n",
    "\n",
    "#### **Neuroscience**\n",
    "- **Time Scale**: Use TR-appropriate kernels (Gaussian with var=25-100 for fMRI)\n",
    "- **Networks**: Focus on within-network and between-network connectivity\n",
    "- **Validation**: Compare with known anatomical connections\n",
    "\n",
    "#### **Economics/Finance**\n",
    "- **Time Scale**: Short-term (daily) vs. long-term (quarterly) correlations\n",
    "- **Events**: Account for market events and volatility clustering\n",
    "- **Sectors**: Analyze sector-specific vs. market-wide correlations\n",
    "\n",
    "#### **Climate Science**\n",
    "- **Seasonality**: Account for strong seasonal patterns\n",
    "- **Geography**: Consider spatial relationships between locations\n",
    "- **Trends**: Separate long-term trends from short-term variations\n",
    "\n",
    "#### **Social Sciences**\n",
    "- **Groups**: Analyze within-group vs. between-group dynamics\n",
    "- **Influence**: Consider social influence and contagion effects\n",
    "- **Events**: Account for social events and interactions\n",
    "\n",
    "### General Best Practices:\n",
    "\n",
    "1. **Choose Appropriate Time Scales**: Match your kernel function to the temporal dynamics of your domain\n",
    "2. **Validate with Known Structure**: Use synthetic data or known relationships to validate your approach\n",
    "3. **Consider Multiple Scales**: Analyze both short-term and long-term correlations\n",
    "4. **Account for Confounds**: Control for known confounding variables in your domain\n",
    "5. **Statistical Testing**: Always perform appropriate statistical tests and corrections\n",
    "6. **Visualize Results**: Create domain-appropriate visualizations\n",
    "7. **Cross-Validation**: Use multiple datasets or time periods to validate findings\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Adapt these examples to your specific research question\n",
    "- Explore higher-order correlations for deeper insights\n",
    "- Consider multi-subject or multi-site analyses\n",
    "- Investigate graph-theoretic measures for network analysis\n",
    "- Develop domain-specific statistical tests and validation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison across domains\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Domain comparison data\n",
    "domains = ['Neuroscience', 'Economics', 'Climate', 'Social']\n",
    "example_correlations = [\n",
    "    network_measures['DMN_within'],  # Neuroscience\n",
    "    avg_corr_short,  # Economics\n",
    "    seasonal_relationships['Arctic_Temp_Precip'],  # Climate\n",
    "    network_measures['Group_A_within']  # Social\n",
    "]\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'purple']\n",
    "\n",
    "for i, (domain, correlation, color) in enumerate(zip(domains, example_correlations, colors)):\n",
    "    # Normalize time axes for comparison\n",
    "    time_axis = np.linspace(0, 1, len(correlation))\n",
    "    axes[i].plot(time_axis, correlation, color=color, linewidth=2)\n",
    "    axes[i].set_title(f'{domain}\\nDynamic Correlation Example', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Normalized Time')\n",
    "    axes[i].set_ylabel('Correlation')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_ylim([-1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TUTORIAL COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou now know how to apply dynamic correlation analysis to:\")\n",
    "print(\"✓ Neuroscience: Brain network connectivity\")\n",
    "print(\"✓ Economics: Market correlation dynamics\")\n",
    "print(\"✓ Climate Science: Environmental relationships\")\n",
    "print(\"✓ Social Sciences: Social network dynamics\")\n",
    "print(\"\\nKey skills acquired:\")\n",
    "print(\"• Domain-specific data simulation\")\n",
    "print(\"• Appropriate time scale selection\")\n",
    "print(\"• Multi-scale correlation analysis\")\n",
    "print(\"• Network structure analysis\")\n",
    "print(\"• Statistical validation methods\")\n",
    "print(\"• Domain-specific visualization techniques\")\n",
    "print(\"\\nReady to apply these techniques to your own research!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}