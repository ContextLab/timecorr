{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `timecorr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`timecorr` is used to approximate dynamic high-order correlations.  There are two steps to `timecorr`:\n",
    "\n",
    "1.  Calculate dynamic correlations  \n",
    "2.  Dimensionally reduce back to the original size of the data\n",
    "\n",
    "By repeating these steps, you can approximate higher-order correlations in a computationally tractable way.  Although both of these steps can be accomplished in just a single, we'll go through and break it down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timecorr as tc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate some data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll use the built in simulation function to simulate some timeseries. By default, the `simulate_data` function will return a 100 samples from 1 subject, using ramping data generation function with 10 features and 5 blocks, but you can specify the number of time samples with `T`, the number of subjects with `S`, and number of features with `K`.  You can also set a random seed to get consistent results across simulations.  If you want further information on simulating data, check out the simulate API page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 1 subject's timeseries \n",
    "sim_1 = tc.simulate_data(S=1, T=200, K=300, set_random_seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output for 1 subject is an array\n",
    "print('shape : ' + str(np.shape(sim_1)))\n",
    "print('type : ' + str(type(sim_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 3 subjects' timeseries\n",
    "sim_3 = tc.simulate_data(S=3, T=200, K=300, set_random_seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output for 3 subjects is a list of arrays\n",
    "print('shape : ' + str(np.shape(sim_3)))\n",
    "print('type : ' + str(type(sim_3)))\n",
    "print('type for sim_3[0] : ' + str(type(sim_3[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate dynamic correlations\n",
    "\n",
    "Now that we have a list of arrays of simulated timeseries data, we can start using `timecorr`.  Let's start by going over the way we calculate dynamic correlations. We use a kernel based approach, and you can specify but the type of kernel with `weights_function` and the width with `weights_params` that you want to use to calculate the correlations.  \n",
    "\n",
    "For this example, we're going to use a gaussian kernel and a width of 5.  Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify kernel:\n",
    "width = 5\n",
    "gaussian = {'name': 'Gaussian', 'weights': tc.gaussian_weights, 'params': {'var': width}}\n",
    "\n",
    "# calcuate the dynamic correlations use a gaussian kernel and width of 5 for 1 simulate subject\n",
    "vec_corrs = tc.timecorr(sim_1, weights_function=gaussian['weights'], weights_params=gaussian['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`timecorr` returns a vectorized version of the correlation matrices. Specifically, the upper triangle of correlation matrices. If you want the full correlation matrices, use the `vec2mat` function. Also, `mat2vec` converts them back to the vectorized version.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns moment-by-moment correlations, but just the upper triangle for the matrices\n",
    "print('vectorized shape : ' + str(np.shape(vec_corrs)))\n",
    "\n",
    "# use the vec2mat function to convert vectorized correlations to moment-by-moment full correlations \n",
    "mat_corrs = tc.vec2mat(vec_corrs)\n",
    "\n",
    "# return the dynamic full correlations\n",
    "print('matrix shape : ' + str(np.shape(mat_corrs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot one of these full correlation matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(mat_corrs[:, :, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's now calculate the dynamic correlations for for the 3 simulated subjects. The default `cfun` calculates a continuous verison of Inter-Subject Functional Connectivity (Simony et al. 2017). If only one data array is passed (rather than a list), the default cfun returns the moment-by-moment correlations for that array. The default for the `combine` function is none, but for this example we'll use `corrmean_combine` which calcuates the average correlations across matrices.  For more information on the different function options, please check out the API documenation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcuate the dynamic isfc correlations use a Laplace kernel \n",
    "# and width of 10 for 3 simulated subjects, and take the element-wise average correlations across matrices.\n",
    "width = 10\n",
    "laplace = {'name': 'Laplace', 'weights': tc.laplace_weights, 'params': {'scale': width}}\n",
    "\n",
    "dyna_corrs = tc.timecorr(sim_3, combine=tc.corrmean_combine, \n",
    "                         weights_function=laplace['weights'], weights_params=laplace['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, this returns the vectorized version of the dynamic correlations\n",
    "print('vectorized shape : ' + str(np.shape(dyna_corrs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher order correlations\n",
    "\n",
    "Ok, now that we've gone over how to calculate dynamic correlations, let's walk through reducing the correlations back to the original size of the data using the `rfun` parameter.  Again, you have several options.  If you want more information, please checkout the API documentation.  \n",
    "\n",
    "The default for `rfun` is `None`, which we used for calculating the dynamic correlations, but in this example we'll use `PCA`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate the dynamic isfc correlation, using a Laplace kernel, width 10, and reducing using PCA\n",
    "width = 10\n",
    "laplace = {'name': 'Laplace', 'weights': tc.laplace_weights, 'params': {'scale': width}}\n",
    "\n",
    "dyna_corrs_reduced = tc.timecorr(sim_3, rfun='PCA', \n",
    "                                 weights_function=laplace['weights'], weights_params=laplace['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns the approximated dynamic correlations the same size as the original data\n",
    "print('original shape : ' + str(np.shape(sim_3)))\n",
    "print('reduced shape : ' + str(np.shape(dyna_corrs_reduced)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate higher-order correlations, you can repeat this process up to any order you want.  For example, if we want to calculate correlations up to the second order, we repeat this process twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_0 = sim_3\n",
    "\n",
    "order_1 = tc.timecorr(order_0, rfun='PCA', weights_function=laplace['weights'], weights_params=laplace['params'])\n",
    "\n",
    "order_2 = tc.timecorr(order_1, rfun='PCA', weights_function=laplace['weights'], weights_params=laplace['params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, and that's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
