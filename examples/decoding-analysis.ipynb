{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp, sqrt, pi\n",
    "from scipy.spatial.distance import squareform, cdist\n",
    "from multiprocessing import Pool,cpu_count\n",
    "\n",
    "def coefficient_generation(timepoint):\n",
    "    '''\n",
    "    Given the guassian array generated in the main function, this function calculates the Gaussian coefficients for each timepoint\n",
    "\n",
    "    Input:\n",
    "        timepoint: the timepoint for which the coeficients are generated\n",
    "\n",
    "    Return:\n",
    "        The sum and a tiled version of the coefficient array\n",
    "    '''\n",
    "    coefficient = gaussian_array[(time_len-1-timepoint):(2*time_len-1-timepoint)]\n",
    "    return np.tile(coefficient,[activations_len,1]), np.sum(coefficient)\n",
    "\n",
    "def isfc_helper(subj):\n",
    "    '''\n",
    "    Helper function to calculate the dynamic correlation matrix for each subject\n",
    "\n",
    "    Input:\n",
    "        subj: the index of the subjec to calculate the dynamic correlation matrix for\n",
    "\n",
    "    Return:\n",
    "        A time_len x activations_len x activations_len matrix\n",
    "    '''\n",
    "    # helper method to calculate correlation matrices at each timepoint\n",
    "    def isfc_timepoint_helper(timepoint):\n",
    "        # normalize activations, summing  over other subjects and calcualte standard deviations\n",
    "        normalized_activations = activations[subj] - np.tile(np.reshape(np.sum(np.multiply(coefficients[timepoint],activations[subj]),1),[activations_len,1]),[1,time_len])/coefficients_sum[timepoint]\n",
    "        normalized_sum_activations = activations_sum[subj] - np.tile(np.reshape(np.sum(np.multiply(coefficients[timepoint],activations_sum[subj]),1),[activations_len,1]),[1,time_len])/coefficients_sum[timepoint]\n",
    "        sigma_activations  = np.sqrt(np.sum(np.multiply(coefficients[timepoint], np.square(normalized_activations)),1)/coefficients_sum[timepoint])\n",
    "        sigma_activations_sum = np.sqrt(np.sum(np.multiply(coefficients[timepoint], np.square(normalized_sum_activations)),1)/coefficients_sum[timepoint])\n",
    "        normalized_activations = np.divide(normalized_activations,np.tile(np.reshape(sigma_activations,[activations_len,1]),[1,time_len]))\n",
    "        normalized_sum_activations = np.divide(normalized_sum_activations,np.tile(np.reshape(sigma_activations_sum,[activations_len,1]),[1,time_len]))\n",
    "\n",
    "        return np.dot(np.multiply(np.tile(coefficients[timepoint,0],[activations_len,1]),normalized_activations),normalized_sum_activations.T)/coefficients_sum[timepoint]\n",
    "\n",
    "    return np.array(map(isfc_timepoint_helper, range(time_len)))\n",
    "\n",
    "def isfc(multi_activations, var=None):\n",
    "    '''\n",
    "    Function to calculate the ISFC for a multi-subject fRMI dataset\n",
    "\n",
    "    Input:\n",
    "        multi_activations: a subject_num x voxel_num x time_len numpy matrix containing the fRMI data   of multiple subjects\n",
    "\n",
    "        var: The variance of the Gaussian distribution used to represent the influence of neighboring timepoints on calculation of correlation at the current timepoint in timecorr\n",
    "\n",
    "    Return:\n",
    "        A time_len x (voxel_num^2-voxel_num)/2 dimension matrix containing the ISFC of the input fRMI dataset\n",
    "    '''\n",
    "    # reference global variables to be used in multiprocessing helper functions\n",
    "    global coefficients, activations_sum, coefficients_sum, activations\n",
    "    global gaussian_array, time_len, subj_num, activations_len\n",
    "    # assign initial parameters\n",
    "    subj_num, activations_len, time_len = multi_activations.shape[0],multi_activations.shape[1],multi_activations.shape[2]\n",
    "    if var is None:\n",
    "        gaussian_variance = min(time_len, 1000)\n",
    "    else:\n",
    "        gaussian_variance = var\n",
    "\n",
    "    coefficients_sum = np.zeros(time_len)\n",
    "    correlations= np.zeros([subj_num, time_len,activations_len,activations_len])\n",
    "    correlations_vector = np.zeros([time_len,(activations_len * (activations_len-1) / 2)])\n",
    "    coefficients = np.zeros([time_len, activations_len,time_len])\n",
    "    gaussian_array = np.array([exp(-timepoint**2/2/gaussian_variance)/sqrt(2*pi*gaussian_variance) for timepoint in range(-time_len+1,time_len)])\n",
    "    activations = np.array(multi_activations)\n",
    "\n",
    "    # generate the gaussian coefficients\n",
    "    for timepoint in range(time_len):\n",
    "        coefficients[timepoint], coefficients_sum[timepoint] = coefficient_generation(timepoint)\n",
    "\n",
    "    # create a matrix that, for each subject, contains the sum of the data for all other subjects\n",
    "    activations_sum = (np.tile(np.sum(activations,0),[subj_num,1,1]) - activations)/(subj_num-1.0)\n",
    "\n",
    "    # calculate the correlations for each timepoint for each subject\n",
    "    p = Pool(min(cpu_count()-1,subj_num))\n",
    "    correlations = np.array(p.map(isfc_helper,range(subj_num)))\n",
    "    p.terminate()\n",
    "\n",
    "    # normalize and average the correlation matrix\n",
    "    correlations_mean = np.mean(0.5*(np.log(1e-5+1+correlations) - np.log(1e-5+1-correlations)),0)\n",
    "    correlations_mean = correlations_mean+np.swapaxes(correlations_mean,1,2)\n",
    "    correlations_mean =  (np.exp(correlations_mean) - 1)/(np.exp(correlations_mean) + 1)\n",
    "\n",
    "    # transform into reverse squareform\n",
    "    for i in range(time_len):\n",
    "        correlations_vector[i] = squareform(correlations_mean[i,:,:],checks=False)\n",
    "\n",
    "    return correlations_vector\n",
    "\n",
    "\n",
    "def wcorr_helper(timepoint):\n",
    "    '''\n",
    "    Helper function to calculate the dynamic correlation at each timepoint\n",
    "\n",
    "    Input:\n",
    "        timepoint: the timepoint at which to calculate the correlationi matrix\n",
    "\n",
    "    Ouput:\n",
    "        A activations_len x activations_len matrix containing the correlation at the input timepoint\n",
    "    '''\n",
    "    # generate coefficients\n",
    "    coefficient_tiled, coefficient_sum = coefficient_generation(timepoint)\n",
    "\n",
    "    # normalize activations and calculate standard deviations\n",
    "    normalized_activations = activations - np.tile(np.reshape(np.sum(np.multiply(coefficient_tiled,activations),1),[activations_len,1]),[1,time_len])/coefficient_sum\n",
    "    sigma  = np.sqrt(np.sum(np.multiply(coefficient_tiled, np.square(normalized_activations)),1)/coefficient_sum)\n",
    "    normalized_activations = np.divide(normalized_activations,np.tile(np.reshape(sigma,[activations_len,1]),[1,time_len]))\n",
    "\n",
    "    return squareform(np.dot(np.multiply(coefficient_tiled,normalized_activations),normalized_activations.T)/coefficient_sum, checks=False)\n",
    "\n",
    "def wcorr(single_activations, var=None):\n",
    "    '''\n",
    "    Function to calculate the ISFC for a single-subject fRMI dataset\n",
    "\n",
    "    Input:\n",
    "        single_activations: a voxel_num x time_len numpy matrix containing the fRMI data of a single subject\n",
    "\n",
    "        var: The variance of the Gaussian distribution used to represent the influence of neighboring timepoints on calculation of correlation at the current timepoint in timecorr\n",
    "\n",
    "    Return:\n",
    "        A time_len x (voxel_num^2-voxel_num)/2 dimension matrix containing the ISFC of the input fRMI dataset\n",
    "    '''\n",
    "    # reference global paramters for multiprocessing\n",
    "    global gaussian_array, activations, time_len, activations_len\n",
    "    # assign initial parameters\n",
    "    activations = single_activations\n",
    "    activations_len, time_len= activations.shape\n",
    "    if var is None:\n",
    "        gaussian_variance = min(time_len, 1000)\n",
    "    else:\n",
    "        gaussian_variance = var\n",
    "\n",
    "    # generate gaussian coefficients\n",
    "    gaussian_array = np.array([exp(-timepoint**2/2/gaussian_variance)/sqrt(2*pi*gaussian_variance) for timepoint in range(-time_len+1,time_len)])\n",
    "\n",
    "    # using multiprocessing to calculate correlations at each timepoint\n",
    "    p = Pool(cpu_count()-1)\n",
    "    correlations_vectors = np.array(p.map(wcorr_helper, range(time_len)))\n",
    "    p.terminate()\n",
    "\n",
    "    return correlations_vectors\n",
    "\n",
    "def timecorr_smoothing(single_activations, var=None):\n",
    "    global gaussian_array, activations, time_len, activations_len\n",
    "    activations = single_activations\n",
    "    activations_len, time_len= activations.shape\n",
    "    smoothed_activations = np.zeros([time_len,activations_len])\n",
    "    if var is None:\n",
    "        gaussian_variance = min(time_len, 1000)\n",
    "    else:\n",
    "        gaussian_variance = var\n",
    "    # generate gaussian coefficients\n",
    "    gaussian_array = np.array([exp(-timepoint**2/2/gaussian_variance)/sqrt(2*pi*gaussian_variance) for timepoint in range(-time_len+1,time_len)])\n",
    "    for timepoint in range(time_len):\n",
    "        coefficient_tiled, coefficient_sum = coefficient_generation(timepoint)\n",
    "        smoothed_activations[timepoint,:] = np.sum(np.multiply(coefficient_tiled,activations),1)/coefficient_sum\n",
    "\n",
    "    return smoothed_activations\n",
    "\n",
    "def sliding_window(activations, window_length):\n",
    "    '''\n",
    "    Sliding window approach to calculate dynamic correlations for single subject\n",
    "\n",
    "    Input:\n",
    "        activations: activations_len x time_len matrix containing fRMI for a single subject\n",
    "\n",
    "        window_length: length of the window to use to calculate correlation at a timepoint\n",
    "\n",
    "    Return:\n",
    "        A time_len x (voxel_num^2-voxel_num)/2 dimension matrix containing the dynamic correlations of the input fRMI dataset\n",
    "    '''\n",
    "    activations_len, time_len = activations.shape\n",
    "    time_len -= window_length-1\n",
    "    correlations = np.zeros([time_len,activations_len,activations_len])\n",
    "    correlations_vector = np.zeros([time_len,(activations_len * (activations_len-1) / 2)])\n",
    "\n",
    "    for timepoint in range(time_len):\n",
    "        correlations[timepoint] = np.corrcoef(activations[:,timepoint:(timepoint+window_length)])\n",
    "        correlations_vector[timepoint] = squareform(correlations[timepoint,:,:],checks=False)\n",
    "\n",
    "    return correlations_vector\n",
    "\n",
    "def sliding_window_smoothing(activations, window_length):\n",
    "    activations_len, time_len = activations.shape\n",
    "    time_len -= window_length-1\n",
    "    smoothed_activations = np.zeros([time_len,activations_len])\n",
    "    for timepoint in range(time_len):\n",
    "        smoothed_activations[timepoint,:] = np.mean(activations[:,timepoint:(timepoint+window_length)],1)\n",
    "\n",
    "    return smoothed_activations\n",
    "\n",
    "def sliding_window_isfc(activations, window_length):\n",
    "    '''\n",
    "    Sliding window approach to calculate dynamic correlations for multiple subjects\n",
    "\n",
    "    Input:\n",
    "        activations: subject_num x activations_len x time_len matrix containing fRMI for a multiple subjects\n",
    "\n",
    "        window_length: length of the window to use to calculate correlation at a timepoint\n",
    "\n",
    "    Return:\n",
    "        A time_len x (voxel_num^2-voxel_num)/2 dimension matrix containing the ISFC of the input fRMI dataset\n",
    "    '''\n",
    "    activations = np.array(activations)\n",
    "    subj_num, activations_len, time_len= activations.shape[0],activations.shape[1],activations.shape[2]-window_length+1\n",
    "    correlations= np.zeros([subj_num, time_len,activations_len,activations_len])\n",
    "    correlations_vector = np.zeros([time_len,(activations_len * (activations_len-1) / 2)])\n",
    "    activations = np.array(activations)\n",
    "    activations_sum = (np.tile(np.sum(activations,0),[subj_num,1,1]) - activations)/(subj_num-1.0)\n",
    "    for subj in range(subj_num):\n",
    "        for timepoint in range(time_len):\n",
    "            correlations[subj, timepoint] = 1-cdist(activations[subj,:,timepoint:(timepoint+window_length)],activations_sum[subj,:,timepoint:(timepoint+window_length)],\"correlation\")\n",
    "\n",
    "    #normalize and average the correlation matrix\n",
    "    correlations_mean = np.mean(0.5*(np.log(1e-5+1+correlations) - np.log(1e-5+1-correlations)),0)\n",
    "    correlations_mean = correlations_mean+np.swapaxes(correlations_mean,1,2)\n",
    "    correlations_mean =  (np.exp(correlations_mean) - 1)/(np.exp(correlations_mean) + 1)\n",
    "\n",
    "    for i in range(time_len):\n",
    "        correlations_vector[i] = squareform(correlations_mean[i,:,:],checks=False)\n",
    "\n",
    "    return correlations_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/pieman-intact.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d44d1a075fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoding accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"level\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/pieman-intact.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arr_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0myerr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arr_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/pieman-intact.npz'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.suptitle(\"pieman dataset decoding accuracy vs level\",fontsize=15)\n",
    "plt.ylabel(\"decoding accuracy\")\n",
    "plt.xlabel(\"level\")\n",
    "data = np.load(\"../data/pieman-intact.npz\")\n",
    "y = data[\"arr_0\"]\n",
    "yerr = data[\"arr_1\"]\n",
    "plt.errorbar(range(11), y, yerr=yerr, fmt='--.', label = \"intact\")\n",
    "\n",
    "data = np.load(\"../data/pieman-paragraph.npz\")\n",
    "y = data[\"arr_0\"]\n",
    "yerr = data[\"arr_1\"]\n",
    "plt.errorbar(range(11), y, yerr=yerr, fmt='--.', label = \"paragraph\")\n",
    "\n",
    "data = np.load(\"../data/pieman-word.npz\")\n",
    "y = data[\"arr_0\"]\n",
    "yerr = data[\"arr_1\"]\n",
    "plt.errorbar(range(11), y, yerr=yerr, fmt='--.', label = \"word\")\n",
    "\n",
    "data = np.load(\"../data/pieman-resting.npz\")\n",
    "y = data[\"arr_0\"]\n",
    "yerr = data[\"arr_1\"]\n",
    "plt.errorbar(range(11), y, yerr=yerr, fmt='--.', label = \"resting\")\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/forrest.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c31235b0bd54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoding accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"level\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/forrest.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arr_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0myerr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arr_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/forrest.npz'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.suptitle(\"forrest dataset decoding accuracy vs level\",fontsize=15)\n",
    "plt.ylabel(\"decoding accuracy\")\n",
    "plt.xlabel(\"level\")\n",
    "data = np.load(\"../data/forrest.npz\")\n",
    "y = data[\"arr_0\"]\n",
    "yerr = data[\"arr_1\"]\n",
    "plt.errorbar(range(11), y, yerr=yerr, fmt='--.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/sherlock.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-95449b764ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoding accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"level\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/sherlock.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arr_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0myerr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arr_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/sherlock.npz'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.suptitle(\"sherlock dataset decoding accuracy vs level\",fontsize=15)\n",
    "plt.ylabel(\"decoding accuracy\")\n",
    "plt.xlabel(\"level\")\n",
    "data = np.load(\"../data/sherlock.npz\")\n",
    "y = data[\"arr_0\"]\n",
    "yerr = data[\"arr_1\"]\n",
    "plt.errorbar(range(11), y, yerr=yerr, fmt='--.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
